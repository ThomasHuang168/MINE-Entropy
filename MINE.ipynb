{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorchtools import EarlyStopping\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "\n",
    "class Mine(nn.Module):\n",
    "    def __init__(self, input_size=2, hidden_size=100):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 1)\n",
    "        nn.init.normal_(self.fc1.weight,std=0.02)\n",
    "        nn.init.constant_(self.fc1.bias, 0)\n",
    "        nn.init.normal_(self.fc2.weight,std=0.02)\n",
    "        nn.init.constant_(self.fc2.bias, 0)\n",
    "        nn.init.normal_(self.fc3.weight,std=0.02)\n",
    "        nn.init.constant_(self.fc3.bias, 0)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output = F.elu(self.fc1(input))\n",
    "        output = F.elu(self.fc2(output))\n",
    "        output = self.fc3(output)\n",
    "        return output\n",
    "\n",
    "def mutual_information(joint, marginal, mine_net):\n",
    "    t = mine_net(joint)\n",
    "    et = torch.exp(mine_net(marginal))\n",
    "    mi_lb = torch.mean(t) - torch.log(torch.mean(et))\n",
    "    return mi_lb, t, et\n",
    "\n",
    "def learn_mine(batch, mine_net, mine_net_optim,  ma_et, ma_rate=0.01):\n",
    "    # batch is a tuple of (joint, marginal)\n",
    "    joint , marginal = batch\n",
    "    joint = torch.autograd.Variable(torch.FloatTensor(joint))\n",
    "    marginal = torch.autograd.Variable(torch.FloatTensor(marginal))\n",
    "    mi_lb , t, et = mutual_information(joint, marginal, mine_net)\n",
    "    ma_et = (1-ma_rate)*ma_et + ma_rate*torch.mean(et)\n",
    "    \n",
    "    # unbiasing use moving average\n",
    "    loss = -(torch.mean(t) - (1/ma_et.mean()).detach()*torch.mean(et))\n",
    "    # use biased estimator\n",
    "#     loss = - mi_lb\n",
    "    \n",
    "    mine_net_optim.zero_grad()\n",
    "    autograd.backward(loss)\n",
    "    mine_net_optim.step()\n",
    "    return mi_lb, ma_et\n",
    "\n",
    "def valid_mine(batch, mine_net):\n",
    "    joint , marginal = batch\n",
    "    joint = torch.autograd.Variable(torch.FloatTensor(joint))\n",
    "    marginal = torch.autograd.Variable(torch.FloatTensor(marginal))\n",
    "    mi_lb , t, et = mutual_information(joint, marginal, mine_net)\n",
    "    return mi_lb\n",
    "    \n",
    "\n",
    "def create_dataset(data, batch_size=100):\n",
    "    if data.shape[0] >= batch_size * 2:\n",
    "        partSize = int(data.shape[0]/2)\n",
    "        indices = list(range(data.shape[0]))\n",
    "        np.random.shuffle(indices)\n",
    "        valid_idx = indices[:partSize]\n",
    "        train_idx = indices[partSize:]\n",
    "        train_data = data[train_idx]\n",
    "        valid_data = data[valid_idx]\n",
    "        return train_data, valid_data\n",
    "    \n",
    "def sample_batch(data, resp, cond, batch_size=100, sample_mode='joint', randomJointIdx=False):\n",
    "    index = np.random.choice(range(data.shape[0]), size=batch_size, replace=False)\n",
    "    batch_joint = data[index]\n",
    "    if randomJointIdx == True:\n",
    "        joint_index = np.random.choice(range(data.shape[0]), size=batch_size, replace=False)\n",
    "        marginal_index = np.random.choice(range(data.shape[0]), size=batch_size, replace=False)\n",
    "        if data.shape[1] == 2:\n",
    "            batch_mar = np.concatenate([batch_joint[joint_index][:,0].reshape(-1,1),\n",
    "                                         batch_joint[marginal_index][:,1].reshape(-1,1)],\n",
    "                                       axis=1)\n",
    "        else:\n",
    "            batch_mar = np.concatenate([batch_joint[joint_index][:,resp].reshape(-1,1),\n",
    "                                         batch_joint[marginal_index][:,cond].reshape(-1,data.shape[1]-1)],\n",
    "                                       axis=1)\n",
    "    else:\n",
    "        marginal_index = np.random.choice(range(batch_joint.shape[0]), size=batch_size, replace=False)\n",
    "        if data.shape[1] == 2:\n",
    "            batch_mar = np.concatenate([batch_joint[:,0].reshape(-1,1),\n",
    "                                         batch_joint[marginal_index][:,1].reshape(-1,1)],\n",
    "                                       axis=1)\n",
    "        else:\n",
    "            batch_mar = np.concatenate([batch_joint[:,resp].reshape(-1,1),\n",
    "                                         batch_joint[marginal_index][:,cond].reshape(-1,data.shape[1]-1)],\n",
    "                                       axis=1)\n",
    "    return batch_joint, batch_mar\n",
    "\n",
    "def train(data, mine_net,mine_net_optim, resp=0, cond=1, batch_size=100\\\n",
    "          , iter_num=int(1e+3), log_freq=int(1e+2)\\\n",
    "          , avg_freq=int(1e+1), verbose=True, patience=20):\n",
    "    # data is x or y\n",
    "    result = list()\n",
    "    ma_et = 1.\n",
    "    \n",
    "    #Early Stopping\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    avg_train_losses = []\n",
    "    avg_valid_losses = []\n",
    "    \n",
    "    earlyStop = EarlyStopping(patience=patience, verbose=True)\n",
    "    trainData, validData = create_dataset(data, batch_size)\n",
    "    for i in range(iter_num):\n",
    "        #get train data\n",
    "        batchTrain = sample_batch(trainData,resp, cond, batch_size=batch_size)\n",
    "        mi_lb, ma_et = learn_mine(batchTrain, mine_net, mine_net_optim, ma_et)\n",
    "        result.append(mi_lb.detach().cpu().numpy())\n",
    "        train_losses.append(result[-1].item())\n",
    "        if verbose and (i+1)%(log_freq)==0:\n",
    "            print(result[-1])\n",
    "        \n",
    "        batchValid = sample_batch(validData,resp, cond, batch_size=batch_size)\n",
    "        mi_lb_valid = valid_mine(batchValid, mine_net)\n",
    "        valid_losses.append(mi_lb_valid.item())\n",
    "        \n",
    "        if (i+1)%(avg_freq)==0:\n",
    "            train_loss = np.average(train_losses)\n",
    "            valid_loss = np.average(valid_losses)\n",
    "            avg_train_losses.append(train_loss)\n",
    "            avg_valid_losses.append(valid_loss)\n",
    "\n",
    "            print_msg = \"[{0}/{1}] train_loss: {2} valid_loss: {3}\".format(i, iter_num, train_loss, valid_loss)\n",
    "            print (print_msg)\n",
    "\n",
    "            train_losses = []\n",
    "            valid_losses = []\n",
    "\n",
    "            earlyStop(valid_loss, mine_net)\n",
    "            if (earlyStop.early_stop):\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "            \n",
    "    mine_net.load_state_dict(torch.load('checkpoint.pt'))\n",
    "    return mine_net, avg_train_losses, avg_valid_losses\n",
    "\n",
    "def ma(a, window_size=100):\n",
    "    if len(a)<=window_size+1:\n",
    "        return [np.mean(a)]\n",
    "    else:\n",
    "        return [np.mean(a[i:i+window_size]) for i in range(0,len(a)-window_size)]\n",
    "\n",
    "def visualizeTrainLogAndSave(train_loss, valid_loss, figName):\n",
    "    # visualize the loss as the network trained\n",
    "    fig = plt.figure(figsize=(10,8))\n",
    "    plt.plot(range(1,len(train_loss)+1),train_loss, label='Training Loss')\n",
    "    plt.plot(range(1,len(valid_loss)+1),valid_loss,label='Validation Loss')\n",
    "\n",
    "    # find position of lowest validation loss\n",
    "    minposs = valid_loss.index(max(valid_loss))+1 \n",
    "    plt.axvline(minposs, linestyle='--', color='r',label='Early Stopping Checkpoint')\n",
    "\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlim(0, len(train_loss)+1) # consistent scale\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(figName, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[399/10000] train_loss: 1.8197149974107742 valid_loss: 1.797738344669342\n",
      "Validation loss increased (1.4116365170478822 --> 1.797738344669342).  Saving model ...\n",
      "[399/10000] train_loss: 0.62601170450449 valid_loss: 0.6379363861680031\n",
      "EarlyStopping counter: 1 out of 20\n",
      "[3999/10000] train_loss: 1.0278712797164917 valid_loss: 1.0004540419578551\n",
      "EarlyStopping counter: 22 out of 50\n",
      "[399/10000] train_loss: 0.3787051616609097 valid_loss: 0.40316330045461657\n",
      "Validation loss increased (0.3928938648104668 --> 0.40316330045461657).  Saving model ...\n",
      "[399/10000] train_loss: 1.89631791472435 valid_loss: 1.9199127280712127\n",
      "Validation loss increased (1.7446011090278626 --> 1.9199127280712127).  Saving model ...\n",
      "[399/10000] train_loss: 1.0332861244678497 valid_loss: 1.0526025819778442\n",
      "Validation loss increased (0.9989173901081085 --> 1.0526025819778442).  Saving model ...\n",
      "[399/10000] train_loss: 1.2871101957559585 valid_loss: 1.2598078787326812\n",
      "Validation loss increased (1.1894041353464126 --> 1.2598078787326812).  Saving model ...\n",
      "[3999/10000] train_loss: 1.379664443731308 valid_loss: 1.4018223285675049\n",
      "Validation loss increased (1.3733256185054779 --> 1.4018223285675049).  Saving model ...\n",
      "[399/10000] train_loss: 1.6347323143482209 valid_loss: 1.6593558019399643\n",
      "Validation loss increased (1.521123584508896 --> 1.6593558019399643).  Saving model ...\n",
      "[499/10000] train_loss: -2.3417851707563387e-07 valid_loss: -9.672304008745414e-07\n",
      "EarlyStopping counter: 4 out of 20\n",
      "[499/10000] train_loss: 1.294693529009819 valid_loss: 1.338325075507164\n",
      "Validation loss increased (1.3331983977556228 --> 1.338325075507164).  Saving model ...\n",
      "[499/10000] train_loss: 0.9976081591844559 valid_loss: 0.9922158640623092\n",
      "EarlyStopping counter: 2 out of 20\n",
      "[499/10000] train_loss: 0.13418419647961854 valid_loss: 0.13469922238960863\n",
      "EarlyStopping counter: 1 out of 20\n",
      "[499/10000] train_loss: 0.3889047610759735 valid_loss: 0.3932812180370092\n",
      "EarlyStopping counter: 1 out of 20\n",
      "[499/10000] train_loss: 1.6508325082063675 valid_loss: 1.6290507924556732\n",
      "EarlyStopping counter: 1 out of 20\n",
      "[499/10000] train_loss: 2.161202268600464 valid_loss: 2.213516067266464[499/10000] train_loss: 0.6784614965319633 valid_loss: 0.6983244544267655\n",
      "Validation loss increased (2.037945009469986 --> 2.213516067266464).  Saving model ...\n",
      "\n",
      "Validation loss increased (0.6718591129779816 --> 0.6983244544267655).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from scipy.stats import randint\n",
    "import DiscreteCondEnt as DC\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def varEntropy(y):\n",
    "    return np.log(np.var(y)*np.pi*2)/2\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "def MSEscorer(clf, X, y):\n",
    "    y_est = clf.predict(X)\n",
    "    return np.log(mean_squared_error(y, y_est)*np.pi*2)/2\n",
    "\n",
    "\n",
    "linReg = LinearRegression()\n",
    "def worker_Train_Mine_cov(input_arg):\n",
    "    cov, MINEsize = input_arg\n",
    "    MINEsize = int(MINEsize)\n",
    "    CVFold = 3\n",
    "    x = np.transpose(np.random.multivariate_normal( mean=[0,0],\n",
    "                                  cov=[[1,cov],[cov,1]],\n",
    "                                 size = MINEsize * 1000))\n",
    "    DE = DC.computeEnt(x, linReg, MSEscorer, varEntropy, CVFold)\n",
    "    MI = DE[1,0] + DE[0,0] - DE[0,1] - DE[1,1]\n",
    "    MI = MI/2\n",
    "    REG = MI\n",
    "    GT = -0.5*np.log(1-cov*cov)\n",
    "    mine_net = Mine()\n",
    "    mine_net_optim = optim.Adam(mine_net.parameters(), lr=1e-3)\n",
    "    mine_net,tl ,vl = train(np.transpose(x),mine_net,mine_net_optim, \\\n",
    "                            verbose=False, batch_size=MINEsize, patience=20)\n",
    "    result_ma = ma(vl)\n",
    "    MINE = result_ma[-1]\n",
    "    filename = \"MINE_Train_Fig_cov={0}_size={1}.png\".format(cov,MINEsize)\n",
    "    visualizeTrainLogAndSave(tl, vl, filename)\n",
    "    return cov, MINE, REG, GT\n",
    "\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "\n",
    "def ParallelWork_cov(Size0):\n",
    "    numThreads = 9\n",
    "    cov = 1 - 0.5**np.arange(numThreads)\n",
    "    size = int(Size0)*np.ones(numThreads)\n",
    "    inputArg = np.concatenate((cov[:,None],size[:,None]),axis=1).tolist()\n",
    "    pool = ThreadPool(numThreads)\n",
    "    results = pool.map(worker_Train_Mine_cov, inputArg)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return results\n",
    "\n",
    "def ParallelWork_size(Cov0):\n",
    "    numThreads = 9\n",
    "    size = int(2)**np.arange(numThreads)\n",
    "    cov = int(Cov0)*np.ones(numThreads)\n",
    "    inputArg = np.concatenate((cov[:,None],size[:,None]),axis=1).tolist()\n",
    "    pool = ThreadPool(numThreads)\n",
    "    results = pool.map(worker_Train_Mine_cov, inputArg)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return results\n",
    "\n",
    "def saveResultFig(figName, GT, Reg, MINE, COV):\n",
    "    fig,ax = plt.subplots()\n",
    "    ax.scatter(COV, MINE, c='b', label='MINE')\n",
    "    ax.scatter(COV, Reg, c='r', label='Regressor')\n",
    "    ax.scatter(COV, GT, c='g', label='Ground Truth')\n",
    "    ax.legend()\n",
    "    fig.savefig(figName, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\n",
      "aaa\n",
      "\n",
      "\n",
      "a\n",
      "aaa\n",
      "a\n",
      "\n",
      "\n",
      "a\n",
      "[99/10000] train_loss: 0.2202606792532606 valid_loss: 0.2062945432093329\n",
      "Validation loss increased (inf --> 0.2062945432093329).  Saving model ...\n",
      "[99/10000] train_loss: 0.10815532956294191 valid_loss: 0.11386982349431492\n",
      "Validation loss increased (inf --> 0.11386982349431492).  Saving model ...\n",
      "[99/10000] train_loss: 0.23578824400461598 valid_loss: 0.24803396705538033\n",
      "Validation loss increased (inf --> 0.24803396705538033).  Saving model ...\n",
      "[99/10000] train_loss: 0.20999955224397127 valid_loss: 0.2180632918626361\n",
      "Validation loss increased (inf --> 0.2180632918626361).  Saving model ...\n",
      "[99/10000] train_loss: 0.05738086808400112 valid_loss: 0.05170364396923105\n",
      "Validation loss increased (inf --> 0.05170364396923105).  Saving model ...\n",
      "[99/10000] train_loss: 0.2016385845115292 valid_loss: 0.20074712165500386\n",
      "Validation loss increased (inf --> 0.20074712165500386).  Saving model ...\n",
      "[99/10000] train_loss: -7.496021873976133e-07 valid_loss: 1.3629136924464546e-06\n",
      "Validation loss increased (inf --> 1.3629136924464546e-06).  Saving model ...\n",
      "[99/10000] train_loss: 0.21898216049223265 valid_loss: 0.21782706158846848\n",
      "Validation loss increased (inf --> 0.21782706158846848).  Saving model ...\n",
      "[99/10000] train_loss: 0.2034295147373996 valid_loss: 0.2007718869042583\n",
      "Validation loss increased (inf --> 0.2007718869042583).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    MINEsize2 = 100\n",
    "\n",
    "    result = np.array(ParallelWork_cov(MINEsize2))\n",
    "\n",
    "    COV2 = result[:,0]\n",
    "    MINE2 = result[:,1]\n",
    "    Reg2 = result[:,2]\n",
    "    GT2 = result[:,3]\n",
    "\n",
    "    filename = \"MINE_Upper_bound_size={0}\".format(MINEsize2)\n",
    "    saveResultFig(filename, GT2, Reg2, MINE2, COV2)\n",
    "    filename = \"MINE_Upper_bound_log_size={0}\".format(MINEsize2)\n",
    "    saveResultFig(filename, GT2, Reg2, MINE2, COV2)\n",
    "\n",
    "    cov = 0.9999\n",
    "\n",
    "    result = np.array(ParallelWork_size(cov))\n",
    "\n",
    "    COV2 = result[:,0]\n",
    "    MINE2 = result[:,1]\n",
    "    Reg2 = result[:,2]\n",
    "    GT2 = result[:,3]\n",
    "\n",
    "    filename = \"MINE_size_Upper_bound_cov={0}\".format(cov)\n",
    "    saveResultFig(filename, GT2, Reg2, MINE2, COV2)\n",
    "    filename = \"MINE_size_log_Upper_bound_cov={0}\".format(cov)\n",
    "    saveResultFig(filename, GT2, Reg2, MINE2, COV2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
